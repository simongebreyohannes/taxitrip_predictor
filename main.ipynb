{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: predicting the trip duration accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "class c:\n",
    "    PURPLE = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data we've been given and divide into reasonable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50000 \n",
    "file_path = 'training_dataset/training_dataset.csv'\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "data_frame = pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data. Remove whatever that's uncessary and check for missing values. Either remove the missing or replace them. \n",
    "\n",
    "But first, check what we're missing and working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values are:  ID                             0\n",
      "vendorid                       0\n",
      "tpep_pickup_datetime           0\n",
      "tpep_dropoff_datetime          0\n",
      "passenger_count          3155336\n",
      "trip_distance                  0\n",
      "ratecodeid               3155336\n",
      "store_and_fwd_flag       3155336\n",
      "pulocationid                   0\n",
      "dolocationid                   0\n",
      "payment_type                   0\n",
      "fare_amount                    0\n",
      "extra                          0\n",
      "mta_tax                        0\n",
      "tip_amount                     0\n",
      "tolls_amount                   0\n",
      "improvement_surcharge          0\n",
      "total_amount                   0\n",
      "congestion_surcharge     3155336\n",
      "airport_fee              3155336\n",
      "duration                       0\n",
      "dtype: int64\n",
      "-------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32701098 entries, 0 to 32701097\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   ID                     object \n",
      " 1   vendorid               int64  \n",
      " 2   tpep_pickup_datetime   object \n",
      " 3   tpep_dropoff_datetime  object \n",
      " 4   passenger_count        float64\n",
      " 5   trip_distance          float64\n",
      " 6   ratecodeid             float64\n",
      " 7   store_and_fwd_flag     object \n",
      " 8   pulocationid           int64  \n",
      " 9   dolocationid           int64  \n",
      " 10  payment_type           int64  \n",
      " 11  fare_amount            float64\n",
      " 12  extra                  float64\n",
      " 13  mta_tax                float64\n",
      " 14  tip_amount             float64\n",
      " 15  tolls_amount           float64\n",
      " 16  improvement_surcharge  float64\n",
      " 17  total_amount           float64\n",
      " 18  congestion_surcharge   float64\n",
      " 19  airport_fee            float64\n",
      " 20  duration               float64\n",
      "dtypes: float64(13), int64(4), object(4)\n",
      "memory usage: 5.1+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data_frame.isnull().sum()\n",
    "print(\"Number of missing values are: \", missing_values) \n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(data_frame.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing a lot of data from these columns: \n",
    "\n",
    "1. passenger_count          3155336\n",
    "\n",
    "2. ratecodeid               3155336\n",
    "\n",
    "3. store_and_fwd_flag       3155336\n",
    "\n",
    "4. congestion_surcharge     3155336\n",
    "\n",
    "5. airport_fee              3155336\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four columns are useless for the objective at hand. We'll drop them. \n",
    "\n",
    "Whilst we're at it we will remove all other columns that are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# data_frame.drop(columns=[\"ratecodeid\", \"payment_type\", \"congestion_surcharge\", \"passenger_count\", \n",
    "#                          \"ID\", \"airport_fee\", \"vendorid\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \n",
    "#                          \"improvement_surcharge\", \"fare_amount\"], inplace=True)\n",
    "data_frame.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values have been addressed as well as useless columns. Now, we'll process the data that is relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       trip_distance  pulocationid  dolocationid  total_amount      duration\n",
      "count   3.270110e+07  3.270110e+07  3.270110e+07  3.270110e+07  3.270110e+07\n",
      "mean    4.908534e+00  1.642556e+02  1.634615e+02  2.785063e+01  1.045067e+03\n",
      "std     4.088654e+02  6.433638e+01  6.960137e+01  8.670093e+01  2.081190e+03\n",
      "min     0.000000e+00  1.000000e+00  1.000000e+00 -2.265450e+03  0.000000e+00\n",
      "25%     1.020000e+00  1.320000e+02  1.130000e+02  1.575000e+01  4.690000e+02\n",
      "50%     1.760000e+00  1.610000e+02  1.620000e+02  2.100000e+01  7.770000e+02\n",
      "75%     3.360000e+00  2.330000e+02  2.340000e+02  3.050000e+01  1.261000e+03\n",
      "max     3.986086e+05  2.650000e+02  2.650000e+02  3.355509e+05  5.854240e+05\n"
     ]
    }
   ],
   "source": [
    "print(data_frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for weird values \\\n",
    "Process data \\\n",
    "Train-test-split \\\n",
    "Choose relevant models\n",
    "- K-neighbour\n",
    "- Random forest\n",
    "- SVC\n",
    "- ... \n",
    "\n",
    "Cross validation \\\n",
    "Draw on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtrip_distance:\u001b[0m\n",
      "trip_distance\n",
      "0.00        619128\n",
      "0.90        414888\n",
      "1.00        412615\n",
      "0.80        405658\n",
      "1.10        401780\n",
      "             ...  \n",
      "57.69            1\n",
      "87.99            1\n",
      "85.83            1\n",
      "4003.25          1\n",
      "96873.59         1\n",
      "Name: count, Length: 8239, dtype: int64\n",
      "\n",
      "\u001b[1mpulocationid:\u001b[0m\n",
      "pulocationid\n",
      "132    1583292\n",
      "161    1520484\n",
      "237    1517936\n",
      "236    1373254\n",
      "162    1128587\n",
      "        ...   \n",
      "199         10\n",
      "5            6\n",
      "105          4\n",
      "99           2\n",
      "110          2\n",
      "Name: count, Length: 263, dtype: int64\n",
      "\n",
      "\u001b[1mdolocationid:\u001b[0m\n",
      "dolocationid\n",
      "236    1423669\n",
      "237    1369419\n",
      "161    1223021\n",
      "230    1039033\n",
      "170     967010\n",
      "        ...   \n",
      "44         143\n",
      "2           66\n",
      "99          10\n",
      "105          5\n",
      "110          2\n",
      "Name: count, Length: 262, dtype: int64\n",
      "\n",
      "\u001b[1mtotal_amount:\u001b[0m\n",
      "total_amount\n",
      " 16.80     424441\n",
      " 12.60     389769\n",
      " 21.00     366010\n",
      " 15.12     240108\n",
      " 15.96     238903\n",
      "            ...  \n",
      " 442.63         1\n",
      " 273.06         1\n",
      "-125.48         1\n",
      " 272.22         1\n",
      "-79.92          1\n",
      "Name: count, Length: 39697, dtype: int64\n",
      "\n",
      "\u001b[1mduration:\u001b[0m\n",
      "duration\n",
      "540.0      38105\n",
      "600.0      37947\n",
      "480.0      37563\n",
      "660.0      37225\n",
      "420.0      36746\n",
      "           ...  \n",
      "44476.0        1\n",
      "55040.0        1\n",
      "30586.0        1\n",
      "33416.0        1\n",
      "88911.0        1\n",
      "Name: count, Length: 19907, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO Check for weird values and eliminate\n",
    "column_names = ['trip_distance', 'pulocationid', 'dolocationid', 'total_amount', 'duration']\n",
    "\n",
    "for column in column_names:\n",
    "    print(f\"{c.BOLD}{column}:{c.END}\")\n",
    "    print(f\"{data_frame[column].value_counts()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all headers are of type int64, which is good because we don't have to convert the values from numerical to categorical. \\\n",
    "It is visible that some trips are extremely long, such as the 96873 (assmued miles) which is unrealistic for a taxi trip in NYC. We can eliminate this, together with any other unrealistic values. \\\n",
    "We can identify these values using some simple statistical analysis.\n",
    "[Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distance outliers: 4184315\n"
     ]
    }
   ],
   "source": [
    "distance_Q1 = data_frame['trip_distance'].quantile(0.25)\n",
    "distance_Q3 = data_frame['trip_distance'].quantile(0.75)\n",
    "distance_IQR = distance_Q3 - distance_Q1\n",
    "\n",
    "distance_lower_bound = distance_Q1 - 1.5 * distance_IQR\n",
    "distance_upper_bound = distance_Q3 + 1.5 * distance_IQR\n",
    "\n",
    "distance_outliers = data_frame[(data_frame['trip_distance'] < distance_lower_bound) | (data_frame['trip_distance'] > distance_upper_bound)]\n",
    "print('number of distance outliers:', len(distance_outliers))\n",
    "\n",
    "df_clean = data_frame[(data_frame['trip_distance'] >= distance_lower_bound) & (data_frame['trip_distance'] <= distance_upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now outliers for distance have been removed. Next we should look for outliers in duration. This is done in the same way to be consistent in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duration outliers: 826846\n"
     ]
    }
   ],
   "source": [
    "duration_Q1 = df_clean['duration'].quantile(0.25)\n",
    "duration_Q3 = df_clean['duration'].quantile(0.75)\n",
    "duration_IQR = duration_Q3 - duration_Q1\n",
    "\n",
    "duration_lower_bound = duration_Q1 - 1.5 * duration_IQR\n",
    "duration_upper_bound = duration_Q3 + 1.5 * duration_IQR\n",
    "\n",
    "duration_outliers = df_clean[(df_clean['duration'] < duration_lower_bound) | (df_clean['duration'] > duration_upper_bound)]\n",
    "print('number of duration outliers:', len(duration_outliers))\n",
    "\n",
    "df_clean = df_clean[(df_clean['duration'] >= duration_lower_bound) & (df_clean['duration'] <= duration_upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that distance and time outliers have been dealt with, next step is to train the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hitsu\\AppData\\Local\\Temp\\ipykernel_674352\\2485550135.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['pickup_hour'] = df_clean['pickup_hour']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df_clean['duration']\n",
    "X = df_clean[['trip_distance']]\n",
    "\n",
    "# Feature engineering to account for pickup time\n",
    "df_clean['pickup_hour'] = pd.to_datetime(df_clean['tpep_pickup_datetime']).dt.hour\n",
    "\n",
    "# TODO: \n",
    "# SettingWithCopyWarning: \n",
    "# A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "# Try using .loc[row_indexer,col_indexer] = value instead\n",
    "# See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "X['pickup_hour'] = df_clean['pickup_hour']\n",
    "\n",
    "# Since we want to use pickup and dropoff locations to account for higher-traffic areas we need to engineer that too. \n",
    "# For this we will use one-hot encoding of the location IDs which will help the model learn the effect of each pickup and dropoff zone.\n",
    "df_clean = pd.get_dummies(df_clean, columns=['pulocationid', 'dolocationid'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = {\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'SVC-linear': LinearSVC(random_state=42, max_iter=2000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'k-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, X, y, cv=10)\n",
    "    cv_results[name] = np.mean(scores)\n",
    "\n",
    "print('Model performance:')\n",
    "for model, accuracy in cv_results.items():\n",
    "    print(f\"{model}: {accuracy: .4f}\")\n",
    "\n",
    "best_model = max(cv_results, key=cv_results.get)\n",
    "print(f\"\\nBest model: {best_model} with accuracy {cv_results[best_model]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
