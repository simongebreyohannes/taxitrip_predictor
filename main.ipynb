{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: predicting the trip duration accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "class c:\n",
    "    PURPLE = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data we've been given and divide into reasonable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, ID: string, vendorid: string, passenger_count: string, trip_distance: string, ratecodeid: string, store_and_fwd_flag: string, pulocationid: string, dolocationid: string, payment_type: string, fare_amount: string, extra: string, mta_tax: string, tip_amount: string, tolls_amount: string, improvement_surcharge: string, total_amount: string, congestion_surcharge: string, airport_fee: string, duration: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AIProject\").config(\"spark.driver.memory\", \"10g\").getOrCreate()\n",
    "data_frame = spark.read.csv('./training_dataset/training_dataset.csv', header=True, inferSchema=True)\n",
    "data_frame.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data. Remove whatever that's uncessary and check for missing values. Either remove the missing or replace them. \n",
    "\n",
    "But first, check what we're missing and working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------+\n",
      "| ID|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|duration|\n",
      "+---+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------+\n",
      "|  0|       0|                   0|                    0|        3155336|            0|   3155336|           3155336|           0|           0|           0|          0|    0|      0|         0|           0|                    0|           0|             3155336|    3155336|       0|\n",
      "+---+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------+\n",
      "\n",
      "Number of missing values are:  DataFrame[ID: bigint, vendorid: bigint, tpep_pickup_datetime: bigint, tpep_dropoff_datetime: bigint, passenger_count: bigint, trip_distance: bigint, ratecodeid: bigint, store_and_fwd_flag: bigint, pulocationid: bigint, dolocationid: bigint, payment_type: bigint, fare_amount: bigint, extra: bigint, mta_tax: bigint, tip_amount: bigint, tolls_amount: bigint, improvement_surcharge: bigint, total_amount: bigint, congestion_surcharge: bigint, airport_fee: bigint, duration: bigint]\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data_frame.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data_frame.columns])\n",
    "missing_values.show()\n",
    "print(\"Number of missing values are: \", missing_values) \n",
    "\n",
    "print(\"-------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing a lot of data from these columns: \n",
    "\n",
    "1. passenger_count          3155336\n",
    "\n",
    "2. ratecodeid               3155336\n",
    "\n",
    "3. store_and_fwd_flag       3155336\n",
    "\n",
    "4. congestion_surcharge     3155336\n",
    "\n",
    "5. airport_fee              3155336\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four columns are useless for the objective at hand. We'll drop them. \n",
    "\n",
    "Whilst we're at it we will remove all other columns that are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, vendorid: int, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, ratecodeid: double, store_and_fwd_flag: string, pulocationid: int, dolocationid: int, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double, duration: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "data_frame.drop(\"ratecodeid\", \"payment_type\", \"congestion_surcharge\", \"passenger_count\", \n",
    "                         \"ID\", \"airport_fee\", \"vendorid\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \n",
    "                         \"improvement_surcharge\", \"fare_amount\")\n",
    "data_frame.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values have been addressed as well as useless columns. Now, we'll process the data that is relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, ID: string, vendorid: string, passenger_count: string, trip_distance: string, ratecodeid: string, store_and_fwd_flag: string, pulocationid: string, dolocationid: string, payment_type: string, fare_amount: string, extra: string, mta_tax: string, tip_amount: string, tolls_amount: string, improvement_surcharge: string, total_amount: string, congestion_surcharge: string, airport_fee: string, duration: string]\n"
     ]
    }
   ],
   "source": [
    "print(data_frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for weird values \\\n",
    "Process data \\\n",
    "Train-test-split \\\n",
    "Choose relevant models\n",
    "- K-neighbour\n",
    "- Random forest\n",
    "- SVC\n",
    "- ... \n",
    "\n",
    "Cross validation \\\n",
    "Draw on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_distance:\n",
      "+-------------+-----+\n",
      "|trip_distance|count|\n",
      "+-------------+-----+\n",
      "|         3.26|21621|\n",
      "|        19.98| 1114|\n",
      "|         0.66|82181|\n",
      "|         8.51| 3817|\n",
      "|         2.86|28584|\n",
      "|         15.5| 4175|\n",
      "|        17.52| 3195|\n",
      "|        17.95| 3212|\n",
      "|         14.9| 3696|\n",
      "|        10.65| 3061|\n",
      "|         9.13| 4141|\n",
      "|         13.4| 3997|\n",
      "|        17.56| 3418|\n",
      "|        26.72|  139|\n",
      "|        40.11|   10|\n",
      "|        23.04|  192|\n",
      "|        12.32| 1291|\n",
      "|        53.82|    4|\n",
      "|         26.7|  390|\n",
      "|        61.95|    2|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "pulocationid:\n",
      "+------------+------+\n",
      "|pulocationid| count|\n",
      "+------------+------+\n",
      "|         148|359744|\n",
      "|         243|  8606|\n",
      "|          31|   290|\n",
      "|         137|349384|\n",
      "|          85|  4369|\n",
      "|         251|    58|\n",
      "|          65| 25298|\n",
      "|         255| 24980|\n",
      "|          53|  1019|\n",
      "|         133|  2413|\n",
      "|          78|  2661|\n",
      "|         155|  3434|\n",
      "|         108|  1715|\n",
      "|         211|270049|\n",
      "|         193| 17808|\n",
      "|          34|  1928|\n",
      "|         126|  1973|\n",
      "|         101|   838|\n",
      "|         115|    80|\n",
      "|          81|  1694|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "dolocationid:\n",
      "+------------+------+\n",
      "|dolocationid| count|\n",
      "+------------+------+\n",
      "|         148|355213|\n",
      "|         243| 65845|\n",
      "|          31|  1854|\n",
      "|         137|409722|\n",
      "|          85|  7724|\n",
      "|         251|   453|\n",
      "|          65| 47973|\n",
      "|         255| 78861|\n",
      "|          53|  5032|\n",
      "|         133|  8875|\n",
      "|          78|  4264|\n",
      "|         155|  9222|\n",
      "|         108|  3254|\n",
      "|         211|285209|\n",
      "|         193| 23925|\n",
      "|          34|  4255|\n",
      "|         101|  3199|\n",
      "|         126|  3734|\n",
      "|         115|   410|\n",
      "|          81|  3055|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "total_amount:\n",
      "+------------+-----+\n",
      "|total_amount|count|\n",
      "+------------+-----+\n",
      "|        15.5|33412|\n",
      "|       12.32|13193|\n",
      "|       23.04|98522|\n",
      "|        14.9|40225|\n",
      "|        13.4|54983|\n",
      "|       -12.3| 2528|\n",
      "|       61.95|  936|\n",
      "|       17.95| 1678|\n",
      "|        26.7|10110|\n",
      "|       37.81|  372|\n",
      "|       58.92| 1168|\n",
      "|        2.86|  215|\n",
      "|       10.65|  597|\n",
      "|       58.51| 1102|\n",
      "|       95.57|   17|\n",
      "|       78.75| 1021|\n",
      "|      -80.94| 2538|\n",
      "|       41.42|  273|\n",
      "|      -14.56|   37|\n",
      "|       45.43| 1372|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "duration:\n",
      "+--------+-----+\n",
      "|duration|count|\n",
      "+--------+-----+\n",
      "|   769.0|22868|\n",
      "|   934.0|18921|\n",
      "|   558.0|28371|\n",
      "|  1051.0|15493|\n",
      "|   305.0|24379|\n",
      "|   496.0|28832|\n",
      "|   596.0|27507|\n",
      "|   299.0|24349|\n",
      "|  4800.0|  215|\n",
      "|   692.0|25175|\n",
      "|  1761.0| 5156|\n",
      "|  2862.0| 1279|\n",
      "|  4142.0|  373|\n",
      "|  2815.0| 1308|\n",
      "|  3597.0|  696|\n",
      "|  3901.0|  481|\n",
      "|  6433.0|   26|\n",
      "|  2734.0| 1407|\n",
      "| 11935.0|    2|\n",
      "|  5360.0|   94|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO Check for weird values and eliminate\n",
    "column_names = ['trip_distance', 'pulocationid', 'dolocationid', 'total_amount', 'duration']\n",
    "\n",
    "for column in column_names:\n",
    "    print(f\"{column}:\")\n",
    "    data_frame.groupBy(col(column)).count().show()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all headers are of type int64, which is good because we don't have to convert the values from numerical to categorical. \\\n",
    "It is visible that some trips are extremely long, such as the 96873 (assmued miles) which is unrealistic for a taxi trip in NYC. We can eliminate this, together with any other unrealistic values. \\\n",
    "We can identify these values using some simple statistical analysis.\n",
    "[Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distance outliers: 4249424\n"
     ]
    }
   ],
   "source": [
    "# Compute Q1, Q3, and IQR using approxQuantile\n",
    "quantiles = data_frame.approxQuantile(\"trip_distance\", [0.25, 0.75], 0.01)  # 0.01 is the error tolerance\n",
    "\n",
    "distance_Q1 = quantiles[0]  # 25th percentile\n",
    "distance_Q3 = quantiles[1]  # 75th percentile\n",
    "distance_IQR = distance_Q3 - distance_Q1\n",
    "\n",
    "# Compute lower and upper bounds for outlier detection\n",
    "distance_lower_bound = distance_Q1 - 1.5 * distance_IQR\n",
    "distance_upper_bound = distance_Q3 + 1.5 * distance_IQR\n",
    "\n",
    "# Identify outliers\n",
    "distance_outliers = data_frame.filter((col(\"trip_distance\") < distance_lower_bound) | (col(\"trip_distance\") > distance_upper_bound))\n",
    "\n",
    "# Count the number of outliers\n",
    "outlier_count = distance_outliers.count()\n",
    "print(f\"Number of distance outliers: {outlier_count}\")\n",
    "\n",
    "# Filter out outliers to create a clean DataFrame\n",
    "df_clean = data_frame.filter((col(\"trip_distance\") >= distance_lower_bound) & (col(\"trip_distance\") <= distance_upper_bound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now outliers for distance have been removed. Next we should look for outliers in duration. This is done in the same way to be consistent in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distance outliers: 828403\n",
      "+-------------+-------------+-----------------+\n",
      "|max(duration)|min(duration)|    avg(duration)|\n",
      "+-------------+-------------+-----------------+\n",
      "|       1956.0|          0.0|744.1257937917635|\n",
      "+-------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute Q1, Q3, and IQR using approxQuantile\n",
    "quantiles = df_clean.approxQuantile(\"duration\", [0.25, 0.75], 0.01)\n",
    "duration_Q1 = quantiles[0]\n",
    "duration_Q3 = quantiles[1]\n",
    "duration_IQR = duration_Q3 - duration_Q1\n",
    "\n",
    "# Compute lower and upper bounds\n",
    "duration_lower_bound = duration_Q1 - 1.5 * duration_IQR\n",
    "duration_upper_bound = duration_Q3 + 1.5 * duration_IQR\n",
    "\n",
    "# Identify outliers\n",
    "duration_outliers = df_clean.filter((col(\"duration\") < duration_lower_bound) | (col(\"duration\") > duration_upper_bound))\n",
    "\n",
    "# Count the number of outliers\n",
    "outlier_count = duration_outliers.count()\n",
    "print(f\"Number of distance outliers: {outlier_count}\")\n",
    "\n",
    "# Filter out outliers to create a clean DataFrame\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"duration\") >= duration_lower_bound) &\n",
    "    (col(\"duration\") <= duration_upper_bound) &\n",
    "    (col(\"duration\").isNotNull())  # Ensure no NULL values\n",
    ")\n",
    "\n",
    "\n",
    "df_clean.selectExpr(\"max(duration)\", \"min(duration)\", \"avg(duration)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that distance and time outliers have been dealt with, next step is to train the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Decision Tree Regressor: RMSE = 0.0000\n",
      "Random Forest Regressor: RMSE = 0.0000\n",
      "\n",
      "Best model: Decision Tree Regressor with RMSE = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, hour\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Ensure pickup_hour is extracted correctly\n",
    "df_clean = df_clean.withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# One-Hot Encoding for Pickup and Dropoff Locations\n",
    "indexer_pu = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "indexer_do = StringIndexer(inputCol=\"dolocationid\", outputCol=\"dolocationid_indexed\")\n",
    "\n",
    "encoder_pu = OneHotEncoder(inputCol=\"pulocationid_indexed\", outputCol=\"pulocationid_encoded\")\n",
    "encoder_do = OneHotEncoder(inputCol=\"dolocationid_indexed\", outputCol=\"dolocationid_encoded\")\n",
    "\n",
    "# Feature Assembler: Combines all features into a single column\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"trip_distance\", \"pickup_hour\", \"pulocationid_encoded\", \"dolocationid_encoded\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Train-Test Split\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define regression models\n",
    "regressors = {\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"duration\"),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"duration\", numTrees=100)\n",
    "}\n",
    "\n",
    "# Evaluator for Regression\n",
    "evaluator = RegressionEvaluator(labelCol=\"duration\", metricName=\"rmse\")\n",
    "\n",
    "# Train and Evaluate Models\n",
    "cv_results = {}\n",
    "for name, model in regressors.items():\n",
    "    pipeline = Pipeline(stages=[indexer_pu, indexer_do, encoder_pu, encoder_do, feature_assembler, model])\n",
    "    \n",
    "    trained_model = pipeline.fit(train_data)  # âœ… train_data is now defined\n",
    "\n",
    "    predictions = trained_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    cv_results[name] = rmse\n",
    "\n",
    "# Print Model Performance\n",
    "print(\"Model Performance:\")\n",
    "for model, rmse in cv_results.items():\n",
    "    print(f\"{model}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Best Model Selection (Lowest RMSE)\n",
    "best_model = min(cv_results, key=cv_results.get)\n",
    "print(f\"\\nBest model: {best_model} with RMSE = {cv_results[best_model]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
